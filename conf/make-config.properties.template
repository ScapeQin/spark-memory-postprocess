# Extra space for GC. This should incldue the blow-up caused by the young generatoin
gcBonus=1.5
# Spark's adjustment (spark.stoarge.safetyFraction) presumably for misestimation of sizes.
assumedSlack=0.9

# Minimums in bytes
minShuffleSizePerCore=16777216
minStorageSizePerCore=16777216
minUnrollStorageSizePerCore=4194304

# After satistifying minimum memory requirements, how to divide spare memory
portionExtraStorage=0.45
portionExtraUnrollStorage=0.1
portionExtraShuffle=0.25
portionExtraUnassignedJvm=0.05

# Don't try to deal with non-JVM space, like trying to keep on disk shuffle
# files in page cache.
ignoreNonJvmSpace=false

# Amount to assume partition sizes multiply by. For example, a factor of 2.0
# assumes that partitions will be twice as large as the instrumented run.
#
# Note that no attempt is made to compensate for repartitioning make partition
# sizes more (or less) uneven than in the sampled run.
scalePartitionsBaseFactor=1.0

# If true, assume partition sizes will be inversely proportional to total task
# capacity configured on the cluster and that the sampled run had the base
# task count specified number of tasks. This adjustment will be applied in addition
# to the factor above.
scalePartitionsBasedOnTasks=false
scalePartitionsBaseTaskCount=1
